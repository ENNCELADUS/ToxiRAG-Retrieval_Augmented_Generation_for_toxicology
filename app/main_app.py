"""
ToxiRAG Streamlit Application
Main interface for liver cancer toxicity prediction and experiment planning.
"""

import os
import sys
import streamlit as st
import tempfile
import asyncio
from pathlib import Path
from typing import Optional, List, Dict, Any

# Add project root to Python path for imports
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Local imports (will be created)
try:
    from ingest.ingest_local import ingest_markdown_file
    from retriever.retriever import retrieve_relevant_docs
    from llm.agentic_pipeline import create_agentic_response
    from config.settings import settings
    import_success = True
    # Optional: show success message in sidebar
    # st.sidebar.success("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    import_success = False
    import traceback
    error_details = traceback.format_exc()
    st.error(f"Import failed: {e}")
    st.error(f"Full traceback:\n{error_details}")
    st.info(f"Current working directory: {os.getcwd()}")
    st.info(f"Project root added to path: {project_root}")
    with st.expander("Show Python Path Details"):
        for i, path in enumerate(sys.path):
            st.text(f"{i}: {path}")
    
    # Try to provide specific error information
    if "ingest_markdown_file" in str(e):
        st.error("Specific issue: Cannot import ingest_markdown_file function")
    if "No module named" in str(e):
        st.error("Module not found - check if running from project root directory")

def setup_page_config():
    """Configure Streamlit page settings."""
    st.set_page_config(
        page_title="ToxiRAG-åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„AIè¾…åŠ©åŠ¨ç‰©å®éªŒé¢„æµ‹å¹³å°",
        page_icon="ğŸ§¬",
        layout="wide",
        initial_sidebar_state="expanded"
    )

def setup_sidebar() -> Dict[str, Any]:
    """Setup sidebar with API keys and model selection."""
    st.sidebar.title("ğŸ”§ é…ç½®è®¾ç½®")
    
    # API Keys section
    st.sidebar.subheader("API å¯†é’¥")
    openai_api_key = st.sidebar.text_input(
        "OpenAI API Key",
        type="password",
        value=settings.openai_api_key or "",
        help="ç”¨äº GPT-5 Nano å’Œ OpenAI åµŒå…¥"
    )
    
    google_api_key = st.sidebar.text_input(
        "Google API Key", 
        type="password",
        value=settings.google_api_key or "",
        help="ç”¨äº Gemini 2.5 Flash"
    )
    
    # Model selection
    st.sidebar.subheader("æ¨¡å‹é€‰æ‹©")
    
    # Available models
    available_models = {
        "gpt-5-mini": "openai",
        "gpt-5-nano": "openai", 
        "gemini-2.5-flash": "gemini"
    }
    
    model_list = list(available_models.keys())
    default_index = model_list.index(settings.default_llm_model) if settings.default_llm_model in model_list else 1
    selected_model = st.sidebar.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        model_list,
        index=default_index,
        help="é€‰æ‹©å…·ä½“çš„è¯­è¨€æ¨¡å‹"
    )
    
    # Derive provider from selected model
    llm_provider = available_models[selected_model]
    
    # Embedding provider
    embedding_provider = st.sidebar.selectbox(
        "åµŒå…¥æ¨¡å‹æä¾›å•†",
        ["openai"],  # Only OpenAI embeddings for now
        help="é€‰æ‹©æ–‡æœ¬åµŒå…¥æ¨¡å‹"
    )
    
    # Advanced settings
    st.sidebar.subheader("é«˜çº§è®¾ç½®")
    max_tokens = st.sidebar.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 100, 4000, 2000)
    temperature = st.sidebar.slider("æ¸©åº¦", 0.0, 1.0, 0.1)
    top_k_docs = st.sidebar.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡", 1, 20, 5)
    
    return {
        "openai_api_key": openai_api_key,
        "google_api_key": google_api_key,
        "llm_provider": llm_provider,
        "selected_model": selected_model,
        "embedding_provider": embedding_provider,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_k_docs": top_k_docs
    }

async def run_async_agentic_response(query: str, config: Dict[str, Any], collection_name: str = None, use_reasoning_tools: bool = True):
    """Wrapper to run async agentic response in Streamlit."""
    try:
        if collection_name is None:
            collection_name = settings.collection_name
        response = await create_agentic_response(
            query=query,
            config=config,
            collection_name=collection_name,
            use_reasoning_tools=use_reasoning_tools
        )
        return response
    except Exception as e:
        st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
        return None

def ingest_section(config: Dict[str, Any]):
    """File upload and ingestion section."""
    st.subheader("ğŸ“ æ•°æ®æ‘„å–")
    
    # File upload
    uploaded_file = st.file_uploader(
        "ä¸Šä¼  Markdown çŸ¥è¯†æ–‡ä»¶",
        type=['md', 'txt'],
        help="ä¸Šä¼ åŒ…å«æ¯’ç†å­¦ç ”ç©¶æ•°æ®çš„ Markdown æ–‡ä»¶"
    )
    
    if uploaded_file is not None:
        # Save uploaded file temporarily
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as tmp_file:
            content = uploaded_file.read().decode('utf-8')
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸš€ å¼€å§‹æ‘„å–", key="ingest_button"):
                if not import_success:
                    st.error("âŒ æ‘„å–å¤±è´¥: æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯")
                else:
                    try:
                        with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶..."):
                            # Call ingestion function (async)
                            result = asyncio.run(ingest_markdown_file(
                                file_path=tmp_file_path,
                                collection_name=settings.collection_name
                            ))
                            st.success(f"âœ… æ–‡ä»¶æ‘„å–æˆåŠŸï¼å¤„ç†äº† {result.get('chunks', 0)} ä¸ªæ–‡æ¡£å—")
                    except NameError as e:
                        st.error(f"âŒ æ‘„å–å¤±è´¥: {e}")
                        st.error("å‡½æ•°æœªå®šä¹‰ï¼Œå¯èƒ½æ˜¯å¯¼å…¥é—®é¢˜")
                        st.info("è¯·ç¡®ä¿ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ Streamlit: `streamlit run app/main_app.py`")
                    except Exception as e:
                        st.error(f"âŒ æ‘„å–å¤±è´¥: {str(e)}")
                        st.error("è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’ŒAPIé…ç½®")
                        import traceback
                        st.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
                    finally:
                        # Clean up temp file
                        if os.path.exists(tmp_file_path):
                            os.unlink(tmp_file_path)
        
        with col2:
            # Enhanced file preview with structure analysis
            st.subheader("ğŸ“„ æ–‡ä»¶é¢„è§ˆ")
            
            # Show file metadata
            lines = content.split('\n')
            st.info(f"ğŸ“Š æ–‡ä»¶ä¿¡æ¯: {len(lines)} è¡Œ, {len(content)} å­—ç¬¦")
            
            # Analyze markdown structure
            headers = [line for line in lines[:50] if line.strip().startswith('#')]
            if headers:
                st.write("**æ–‡æ¡£ç»“æ„é¢„è§ˆ:**")
                for header in headers[:10]:  # Show first 10 headers
                    level = len(header) - len(header.lstrip('#'))
                    st.write(f"{'  ' * (level-1)}â€¢ {header.strip('#').strip()}")
                if len(headers) > 10:
                    st.write(f"... è¿˜æœ‰ {len(headers)-10} ä¸ªæ ‡é¢˜")
            
            # Show content preview
            preview_text = content[:800] + "..." if len(content) > 800 else content
            st.text_area("å†…å®¹é¢„è§ˆ", preview_text, height=150, disabled=True)

def gpt5_reasoning_tab(config: Dict[str, Any]):
    """Retrieval and reasoning tab using new agentic pipeline."""
    model_name = config["selected_model"]
    st.header(f"ğŸ¤– æ£€ç´¢ä¸å›ç­”ï¼ˆ{model_name}ï¼‰")
    
    # Check API keys
    if not config["openai_api_key"] and not config["google_api_key"]:
        st.error("æ— æ³•è®¾ç½®æ¨ç†ä»£ç†ã€‚è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®ã€‚")
        return
    
    # Query input
    query = st.text_area(
        "è¯·è¾“å…¥æ‚¨çš„æ¯’ç†å­¦é—®é¢˜:",
        placeholder="ä¾‹å¦‚: è¯·åˆ†ææŸä¸ªTCMåŒ–åˆç‰©å¯¹è‚ç™Œç»†èƒçš„æ¯’æ€§ä½œç”¨æœºåˆ¶...",
        height=100
    )
    
    col1, col2 = st.columns([3, 1])
    
    with col2:
        if st.button("ğŸ” åˆ†æ", key="gpt5_analyze"):
            if not query.strip():
                st.warning("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹")
                return
            
            with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶åˆ†æ..."):
                try:
                    # Use new agentic pipeline
                    response = asyncio.run(run_async_agentic_response(
                        query=query,
                        config=config,
                        collection_name="tcm_tox",
                        use_reasoning_tools=False  # Basic reasoning for this tab
                    ))
                    
                    if response and response.refusal_reason is None:
                        # Display results
                        st.subheader("ğŸ“Š åˆ†æç»“æœ")
                        st.markdown(response.response_text)
                        
                        # Display evidence pack info
                        if response.evidence_pack.citation_ids:
                            st.subheader("ğŸ“‹ è¯æ®å¼•ç”¨")
                            for citation in response.citations:
                                st.markdown(f"- {citation}")
                        
                        # Display retrieved sources
                        with st.expander("ğŸ“š å‚è€ƒæ–‡çŒ®æ¥æº"):
                            for i, doc in enumerate(response.evidence_pack.retrieved_docs, 1):
                                st.write(f"**æ¥æº {i}:** {doc.get('document_title', 'æœªçŸ¥æ ‡é¢˜')}")
                                st.write(f"**èŠ‚æ®µ:** {doc.get('section_type', 'æœªçŸ¥èŠ‚æ®µ')}")
                                st.write(f"**æ‘˜è¦:** {doc.get('content', '')[:200]}...")
                                if doc.get('source_page'):
                                    st.write(f"**é¡µé¢:** {doc.get('source_page')}")
                                st.write("---")
                        
                        # Display confidence and reasoning info
                        st.subheader("ğŸ” åˆ†æä¿¡æ¯")
                        st.write(f"**ç½®ä¿¡åº¦:** {response.confidence_score:.2f}")
                        st.write(f"**æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°:** {len(response.evidence_pack.retrieved_docs)}")
                        st.write(f"**å¼•ç”¨æ•°:** {len(response.citations)}")
                    
                    elif response and response.refusal_reason:
                        st.warning("âš ï¸ åˆ†æå—é™")
                        st.markdown(response.response_text)
                        st.write(f"**åŸå› :** {response.refusal_reason}")
                    
                    else:
                        st.error("åˆ†æå¤±è´¥ï¼Œè¯·é‡è¯•")
                
                except Exception as e:
                    st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
    
    with col1:
        # Show query parameters
        model_display = config['selected_model']
        st.info(f"ğŸ”§ å½“å‰é…ç½®: {model_display} | æ£€ç´¢æ–‡æ¡£æ•°: {config['top_k_docs']} | æ¸©åº¦: {config['temperature']}")

def reasoning_visualization_tab(config: Dict[str, Any]):
    """Reasoning visualization tab with advanced agentic pipeline."""
    embedding_name = settings.openai_embed_model if config["embedding_provider"] == "openai" else config["embedding_provider"]
    st.header(f"ğŸ§  æ¨ç†å¯è§†åŒ–ï¼ˆ{embedding_name}ï¼‰")
    
    # Check API keys
    if not config["openai_api_key"] and not config["google_api_key"]:
        st.error("æ— æ³•è®¾ç½®çŸ¥è¯†ä»£ç†ã€‚è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®ã€‚")
        return
    
    # Query input
    query = st.text_area(
        "è¯·è¾“å…¥å¤æ‚çš„æ¯’ç†å­¦ç ”ç©¶é—®é¢˜:",
        placeholder="ä¾‹å¦‚: æ¯”è¾ƒä¸åŒTCMåŒ–åˆç‰©åœ¨è‚ç™Œæ²»ç–—ä¸­çš„æ¯’æ€§å·®å¼‚ï¼Œå¹¶æä¾›å®éªŒè®¾è®¡æ–¹æ¡ˆ...",
        height=100
    )
    
    col1, col2 = st.columns([3, 1])
    
    with col2:
        show_reasoning = st.checkbox("æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹", value=True)
        if st.button("ğŸ§© æ·±åº¦åˆ†æ", key="reasoning_analyze"):
            if not query.strip():
                st.warning("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹")
                return
            
            with st.spinner("æ­£åœ¨è¿›è¡Œæ·±åº¦æ¨ç†åˆ†æ..."):
                try:
                    # Use advanced agentic pipeline with reasoning tools
                    response = asyncio.run(run_async_agentic_response(
                        query=query,
                        config=config,
                        collection_name=settings.collection_name,
                        use_reasoning_tools=True  # Advanced reasoning for this tab
                    ))
                    
                    if response and response.refusal_reason is None:
                        # Show reasoning steps if requested
                        if show_reasoning and response.reasoning_steps:
                            st.subheader("ğŸ” æ¨ç†è¿‡ç¨‹")
                            reasoning_container = st.container()
                            
                            with reasoning_container:
                                st.markdown("**æ¨ç†æ­¥éª¤:**")
                                for i, step in enumerate(response.reasoning_steps, 1):
                                    st.markdown(f"{i}. {step}")
                        
                        # Display main results
                        st.subheader("ğŸ“‹ åˆ†ææŠ¥å‘Š")
                        st.markdown(response.response_text)
                        
                        # Display evidence and citations
                        if response.evidence_pack.citation_ids:
                            st.subheader("ğŸ“‹ è¯æ®å¼•ç”¨")
                            for citation in response.citations:
                                st.markdown(f"- {citation}")
                        
                        # Advanced analysis metrics
                        st.subheader("ğŸ”¬ åˆ†æè¯¦æƒ…")
                        col_a, col_b, col_c = st.columns(3)
                        
                        with col_a:
                            st.metric("ç½®ä¿¡åº¦", f"{response.confidence_score:.2f}")
                        
                        with col_b:
                            st.metric("è¯æ®æ–‡æ¡£", len(response.evidence_pack.retrieved_docs))
                        
                        with col_c:
                            st.metric("å¼•ç”¨æ•°é‡", len(response.citations))
                        
                        # Query decomposition info
                        st.subheader("ğŸ§© æŸ¥è¯¢åˆ†è§£")
                        st.write("**æŸ¥è¯¢ç±»å‹åˆ†ç±»**: è‡ªåŠ¨è¯†åˆ«å¹¶åˆ†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢")
                        st.write("**è¯æ®æ£€ç´¢**: å»é‡åçš„ç›¸å…³æ–‡æ¡£")
                        st.write("**æ¨ç†åˆæˆ**: åŸºäºè¯æ®çš„ç»“æ„åŒ–åˆ†æ")
                        
                        # Display retrieved sources with advanced info
                        with st.expander("ğŸ“š è¯¦ç»†æ–‡çŒ®æ¥æº"):
                            for i, doc in enumerate(response.evidence_pack.retrieved_docs, 1):
                                st.write(f"**æ–‡æ¡£ {i}:** {doc.get('document_title', 'æœªçŸ¥æ ‡é¢˜')}")
                                st.write(f"**èŠ‚æ®µç±»å‹:** {doc.get('section_type', 'æœªçŸ¥èŠ‚æ®µ')}")
                                if doc.get('vector_score'):
                                    st.write(f"**ç›¸ä¼¼åº¦å¾—åˆ†:** {doc.get('vector_score', 0):.3f}")
                                if doc.get('bm25_score'):
                                    st.write(f"**å…³é”®è¯å¾—åˆ†:** {doc.get('bm25_score', 0):.3f}")
                                if doc.get('combined_score'):
                                    st.write(f"**ç»¼åˆå¾—åˆ†:** {doc.get('combined_score', 0):.3f}")
                                st.write(f"**å†…å®¹æ‘˜è¦:** {doc.get('content', '')[:200]}...")
                                if doc.get('source_page'):
                                    st.write(f"**æºé¡µé¢:** {doc.get('source_page')}")
                                st.write("---")
                    
                    elif response and response.refusal_reason:
                        st.warning("âš ï¸ æ·±åº¦åˆ†æå—é™")
                        st.markdown(response.response_text)
                        st.write(f"**é™åˆ¶åŸå› :** {response.refusal_reason}")
                    
                    else:
                        st.error("æ·±åº¦åˆ†æå¤±è´¥ï¼Œè¯·é‡è¯•")
                
                except Exception as e:
                    st.error(f"æ¨ç†åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
    
    with col1:
        # Advanced reasoning parameters
        model_display = config['selected_model']
        embedding_display = settings.openai_embed_model if config['embedding_provider'] == 'openai' else config['embedding_provider']
        st.info(f"ğŸ”§ é«˜çº§é…ç½®: {model_display} + çŸ¥è¯†æ¨ç†å·¥å…· | åµŒå…¥: {embedding_display}")
        
        # Show reasoning tools info
        with st.expander("â„¹ï¸ é«˜çº§æ¨ç†åŠŸèƒ½"):
            st.markdown("""
            **æ™ºèƒ½æŸ¥è¯¢åˆ†è§£:**
            - ğŸ§  **è‡ªåŠ¨åˆ†ç±»**: æœºåˆ¶/æ¯’æ€§/è®¾è®¡/å¯¹æ¯”/ä¸€èˆ¬
            - ğŸ” **å¤šè§’åº¦æ£€ç´¢**: å­æŸ¥è¯¢å¹¶è¡Œæœç´¢
            - ğŸ“Š **è¯æ®èšåˆ**: å»é‡å’Œç›¸å…³æ€§æ’åº
            
            **æ¨ç†å¢å¼ºåŠŸèƒ½:**
            - ğŸ¤” **ç»“æ„åŒ–æ€è€ƒ**: åˆ†æ­¥åˆ†æé—®é¢˜
            - ğŸ” **çŸ¥è¯†åº“æœç´¢**: æ··åˆæ£€ç´¢ç­–ç•¥
            - ğŸ“Š **è¯æ®è¯„ä¼°**: ç½®ä¿¡åº¦è®¡ç®—
            - ğŸ›¡ï¸ **å®‰å…¨é˜²æŠ¤**: æ‹’ç»å›ç­”ä¸å……åˆ†é—®é¢˜
            
            **é€‚ç”¨å¤æ‚åœºæ™¯:**
            - å¤šåŒ–åˆç‰©å¯¹æ¯”åˆ†æ
            - å®éªŒè®¾è®¡æ–¹æ¡ˆåˆ¶å®š
            - æœºåˆ¶ç ”ç©¶ç»¼åˆè¯„ä¼°
            - å®‰å…¨æ€§é£é™©åˆ†æ
            """)

def main():
    """Main application entry point."""
    setup_page_config()
    
    # Header
    st.title("ğŸ§¬ ToxiRAG-åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„AIè¾…åŠ©åŠ¨ç‰©å®éªŒé¢„æµ‹å¹³å°")
    st.markdown("*åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„AIè¾…åŠ©åŠ¨ç‰©å®éªŒé¢„æµ‹å¹³å°*")
    
    # Sidebar configuration
    config = setup_sidebar()
    
    # Check API keys
    if not config["openai_api_key"] and not config["google_api_key"]:
        st.warning("âš ï¸ è¯·åœ¨ä¾§è¾¹æ é…ç½®è‡³å°‘ä¸€ä¸ªAPIå¯†é’¥ä»¥ç»§ç»­ä½¿ç”¨")
        return
    
    # File ingestion section
    with st.expander("ğŸ“ çŸ¥è¯†åº“ç®¡ç†", expanded=False):
        ingest_section(config)
    
    # Main tabs with dynamic model names
    model_name = config["selected_model"]
    embedding_name = settings.openai_embed_model if config["embedding_provider"] == "openai" else config["embedding_provider"]
    tab1, tab2 = st.tabs([f"ğŸ¤– æ£€ç´¢ä¸å›ç­”ï¼ˆ{model_name}ï¼‰", f"ğŸ§  æ¨ç†å¯è§†åŒ–ï¼ˆ{embedding_name}ï¼‰"])
    
    with tab1:
        gpt5_reasoning_tab(config)
    
    with tab2:
        reasoning_visualization_tab(config)
    
    # Footer
    st.markdown("---")
    st.markdown(
        "ğŸ’¡ **æç¤º**: ä½¿ç”¨æ¨ç†å¯è§†åŒ–åŠŸèƒ½å¯ä»¥çœ‹åˆ°AIçš„æ€è€ƒè¿‡ç¨‹ï¼Œ"
        "é€‚åˆå¤æ‚çš„æ¯’ç†å­¦åˆ†æå’Œå®éªŒè®¾è®¡ã€‚"
    )

if __name__ == "__main__":
    main() 