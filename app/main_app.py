"""
ToxiRAG Streamlit Application
Main interface for liver cancer toxicity prediction and experiment planning.
"""

import os
import streamlit as st
import tempfile
from pathlib import Path
from typing import Optional, List, Dict, Any

# Agno imports for reasoning tools
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.models.google import Gemini
from agno.tools.reasoning import ReasoningTools
from agno.tools.knowledge import KnowledgeTools
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge.text import TextKnowledgeBase
from agno.vectordb.lancedb import LanceDb, SearchType

# Local imports (will be created)
try:
    from ingest.ingest_local import ingest_markdown_file, setup_qdrant_collection
    from retriever.retriever import retrieve_relevant_docs
    from llm.agentic_pipeline import create_agentic_response
except ImportError:
    st.error("Missing local modules. Please ensure ingest/, retriever/, and llm/ modules are implemented.")

def setup_page_config():
    """Configure Streamlit page settings."""
    st.set_page_config(
        page_title="ToxiRAG - è‚ç™Œæ¯’æ€§é¢„æµ‹ç³»ç»Ÿ",
        page_icon="ğŸ§¬",
        layout="wide",
        initial_sidebar_state="expanded"
    )

def setup_sidebar() -> Dict[str, Any]:
    """Setup sidebar with API keys and model selection."""
    st.sidebar.title("ğŸ”§ é…ç½®è®¾ç½®")
    
    # API Keys section
    st.sidebar.subheader("API å¯†é’¥")
    openai_api_key = st.sidebar.text_input(
        "OpenAI API Key",
        type="password",
        value=os.getenv("OPENAI_API_KEY", ""),
        help="ç”¨äº GPT-5 Nano å’Œ OpenAI åµŒå…¥"
    )
    
    google_api_key = st.sidebar.text_input(
        "Google API Key", 
        type="password",
        value=os.getenv("GOOGLE_API_KEY", ""),
        help="ç”¨äº Gemini 2.5 Flash"
    )
    
    # Model selection
    st.sidebar.subheader("æ¨¡å‹é€‰æ‹©")
    llm_provider = st.sidebar.selectbox(
        "LLM æä¾›å•†",
        ["openai", "gemini"],
        help="é€‰æ‹©ä¸»è¦çš„è¯­è¨€æ¨¡å‹æä¾›å•†"
    )
    
    # Embedding provider
    embedding_provider = st.sidebar.selectbox(
        "åµŒå…¥æ¨¡å‹æä¾›å•†",
        ["openai"],  # Only OpenAI embeddings for now
        help="é€‰æ‹©æ–‡æœ¬åµŒå…¥æ¨¡å‹"
    )
    
    # Advanced settings
    st.sidebar.subheader("é«˜çº§è®¾ç½®")
    max_tokens = st.sidebar.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 100, 4000, 2000)
    temperature = st.sidebar.slider("æ¸©åº¦", 0.0, 1.0, 0.1)
    top_k_docs = st.sidebar.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡", 1, 20, 5)
    
    return {
        "openai_api_key": openai_api_key,
        "google_api_key": google_api_key,
        "llm_provider": llm_provider,
        "embedding_provider": embedding_provider,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_k_docs": top_k_docs
    }

def setup_agent(config: Dict[str, Any], agent_type: str = "reasoning") -> Optional[Agent]:
    """Setup Agno agent with reasoning tools."""
    try:
        # Set environment variables
        if config["openai_api_key"]:
            os.environ["OPENAI_API_KEY"] = config["openai_api_key"]
        if config["google_api_key"]:
            os.environ["GOOGLE_API_KEY"] = config["google_api_key"]
        
        # Select model based on provider
        if config["llm_provider"] == "openai":
            model = OpenAIChat(
                id="gpt-5-nano", 
                api_key=config["openai_api_key"],
                max_tokens=config["max_tokens"],
                temperature=config["temperature"]
            )
        else:  # gemini
            model = Gemini(
                id="gemini-2.5-flash",
                api_key=config["google_api_key"],
                max_tokens=config["max_tokens"],
                temperature=config["temperature"]
            )
        
        # Setup tools based on agent type
        if agent_type == "reasoning":
            tools = [ReasoningTools(add_instructions=True)]
        elif agent_type == "knowledge":
            # Setup knowledge base for toxicology data
            knowledge = TextKnowledgeBase(
                vector_db=LanceDb(
                    uri="tmp/toxirag_lancedb",
                    table_name="toxicology_docs",
                    search_type=SearchType.hybrid,
                    embedder=OpenAIEmbedder(
                        id="text-embedding-3-large",
                        api_key=config["openai_api_key"]
                    )
                )
            )
            tools = [KnowledgeTools(
                knowledge=knowledge,
                think=True,
                search=True,
                analyze=True,
                add_few_shot=True
            )]
        else:
            tools = []
        
        agent = Agent(
            model=model,
            tools=tools,
            instructions="""You are a toxicology expert specializing in liver cancer research and Traditional Chinese Medicine (TCM) compounds. 
            Provide detailed, evidence-based responses about toxicity predictions and experimental planning.
            Always cite your sources and explain your reasoning step by step.""",
            show_tool_calls=True,
            markdown=True
        )
        
        return agent
    except Exception as e:
        st.error(f"è®¾ç½®ä»£ç†æ—¶å‡ºé”™: {str(e)}")
        return None

def ingest_section(config: Dict[str, Any]):
    """File upload and ingestion section."""
    st.subheader("ğŸ“ æ•°æ®æ‘„å–")
    
    # File upload
    uploaded_file = st.file_uploader(
        "ä¸Šä¼  Markdown çŸ¥è¯†æ–‡ä»¶",
        type=['md', 'txt'],
        help="ä¸Šä¼ åŒ…å«æ¯’ç†å­¦ç ”ç©¶æ•°æ®çš„ Markdown æ–‡ä»¶"
    )
    
    if uploaded_file is not None:
        # Save uploaded file temporarily
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as tmp_file:
            content = uploaded_file.read().decode('utf-8')
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸš€ å¼€å§‹æ‘„å–", key="ingest_button"):
                try:
                    with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶..."):
                        # Call ingestion function
                        result = ingest_markdown_file(
                            file_path=tmp_file_path,
                            collection_name="tcm_tox"
                        )
                        st.success(f"âœ… æ–‡ä»¶æ‘„å–æˆåŠŸï¼å¤„ç†äº† {result.get('chunks', 0)} ä¸ªæ–‡æ¡£å—")
                except Exception as e:
                    st.error(f"âŒ æ‘„å–å¤±è´¥: {str(e)}")
                finally:
                    # Clean up temp file
                    if os.path.exists(tmp_file_path):
                        os.unlink(tmp_file_path)
        
        with col2:
            # Preview content
            st.text_area("æ–‡ä»¶é¢„è§ˆ", content[:500] + "..." if len(content) > 500 else content, height=100)

def gpt5_reasoning_tab(config: Dict[str, Any]):
    """GPT-5 retrieval and reasoning tab."""
    st.header("ğŸ¤– æ£€ç´¢ä¸å›ç­”ï¼ˆGPT-5ï¼‰")
    
    # Setup reasoning agent
    agent = setup_agent(config, "reasoning")
    if not agent:
        st.error("æ— æ³•è®¾ç½®æ¨ç†ä»£ç†ã€‚è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®ã€‚")
        return
    
    # Query input
    query = st.text_area(
        "è¯·è¾“å…¥æ‚¨çš„æ¯’ç†å­¦é—®é¢˜:",
        placeholder="ä¾‹å¦‚: è¯·åˆ†ææŸä¸ªTCMåŒ–åˆç‰©å¯¹è‚ç™Œç»†èƒçš„æ¯’æ€§ä½œç”¨æœºåˆ¶...",
        height=100
    )
    
    col1, col2 = st.columns([3, 1])
    
    with col2:
        if st.button("ğŸ” åˆ†æ", key="gpt5_analyze"):
            if not query.strip():
                st.warning("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹")
                return
            
            with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶åˆ†æ..."):
                try:
                    # Step 1: Retrieve relevant documents
                    retrieved_docs = retrieve_relevant_docs(
                        query=query,
                        top_k=config["top_k_docs"],
                        collection_name="tcm_tox"
                    )
                    
                    # Step 2: Create context-aware query
                    context = "\n\n".join([doc.get('content', '') for doc in retrieved_docs])
                    enhanced_query = f"""
                    åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„æ¯’ç†å­¦æ–‡çŒ®ä¸Šä¸‹æ–‡ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜:
                    
                    ä¸Šä¸‹æ–‡æ–‡æ¡£:
                    {context}
                    
                    ç”¨æˆ·é—®é¢˜: {query}
                    
                    è¯·æä¾›è¯¦ç»†çš„åˆ†æï¼ŒåŒ…æ‹¬:
                    1. ç›¸å…³çš„æ¯’ç†å­¦æœºåˆ¶
                    2. å®éªŒè®¾è®¡å»ºè®®
                    3. å®‰å…¨æ€§è€ƒè™‘
                    4. å¼•ç”¨çš„è¯æ®æ¥æº
                    """
                    
                    # Step 3: Generate reasoning response
                    response = agent.run(enhanced_query)
                    
                    # Display results
                    st.subheader("ğŸ“Š åˆ†æç»“æœ")
                    st.markdown(response.content)
                    
                    # Display retrieved sources
                    with st.expander("ğŸ“š å‚è€ƒæ–‡çŒ®æ¥æº"):
                        for i, doc in enumerate(retrieved_docs, 1):
                            st.write(f"**æ¥æº {i}:** {doc.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                            st.write(f"**æ‘˜è¦:** {doc.get('content', '')[:200]}...")
                            st.write("---")
                
                except Exception as e:
                    st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
    
    with col1:
        # Show query parameters
        st.info(f"ğŸ”§ å½“å‰é…ç½®: {config['llm_provider'].upper()} | æ£€ç´¢æ–‡æ¡£æ•°: {config['top_k_docs']} | æ¸©åº¦: {config['temperature']}")

def reasoning_visualization_tab(config: Dict[str, Any]):
    """Reasoning visualization tab with OpenAI embeddings."""
    st.header("ğŸ§  æ¨ç†å¯è§†åŒ–ï¼ˆOpenAI åµŒå…¥ï¼‰")
    
    # Setup knowledge agent
    agent = setup_agent(config, "knowledge")
    if not agent:
        st.error("æ— æ³•è®¾ç½®çŸ¥è¯†ä»£ç†ã€‚è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®ã€‚")
        return
    
    # Query input
    query = st.text_area(
        "è¯·è¾“å…¥å¤æ‚çš„æ¯’ç†å­¦ç ”ç©¶é—®é¢˜:",
        placeholder="ä¾‹å¦‚: æ¯”è¾ƒä¸åŒTCMåŒ–åˆç‰©åœ¨è‚ç™Œæ²»ç–—ä¸­çš„æ¯’æ€§å·®å¼‚ï¼Œå¹¶æä¾›å®éªŒè®¾è®¡æ–¹æ¡ˆ...",
        height=100
    )
    
    col1, col2 = st.columns([3, 1])
    
    with col2:
        show_reasoning = st.checkbox("æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹", value=True)
        if st.button("ğŸ§© æ·±åº¦åˆ†æ", key="reasoning_analyze"):
            if not query.strip():
                st.warning("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹")
                return
            
            with st.spinner("æ­£åœ¨è¿›è¡Œæ·±åº¦æ¨ç†åˆ†æ..."):
                try:
                    # Enhanced query for reasoning visualization
                    enhanced_query = f"""
                    ä½œä¸ºæ¯’ç†å­¦ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ·±åº¦åˆ†æ:
                    
                    {query}
                    
                    è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æ:
                    1. é¦–å…ˆæ€è€ƒé—®é¢˜çš„å…³é”®è¦ç´ 
                    2. æœç´¢ç›¸å…³çš„æ¯’ç†å­¦çŸ¥è¯†
                    3. åˆ†æä¸åŒåŒ–åˆç‰©çš„ä½œç”¨æœºåˆ¶
                    4. æ¯”è¾ƒå®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§
                    5. æä¾›å…·ä½“çš„å®éªŒè®¾è®¡å»ºè®®
                    6. æ€»ç»“å…³é”®å‘ç°å’Œå»ºè®®
                    """
                    
                    # Generate response with reasoning tools
                    if show_reasoning:
                        # Show intermediate reasoning steps
                        st.subheader("ğŸ” æ¨ç†è¿‡ç¨‹")
                        reasoning_container = st.container()
                        
                        response = agent.run(
                            enhanced_query,
                            stream=True,
                            show_full_reasoning=True
                        )
                        
                        with reasoning_container:
                            st.markdown("**æ¨ç†æ­¥éª¤:**")
                            # Note: Actual reasoning visualization would need custom streaming handler
                            st.info("æ¨ç†å·¥å…·æ­£åœ¨åˆ†æé—®é¢˜...")
                    else:
                        response = agent.run(enhanced_query)
                    
                    # Display final results
                    st.subheader("ğŸ“‹ åˆ†ææŠ¥å‘Š")
                    st.markdown(response.content)
                    
                    # Additional insights
                    st.subheader("ğŸ’¡ å…³é”®æ´å¯Ÿ")
                    insights_query = f"åŸºäºä¸Šè¿°åˆ†æï¼Œè¯·æ€»ç»“3ä¸ªæœ€é‡è¦çš„æ¯’ç†å­¦æ´å¯Ÿå’Œå®éªŒå»ºè®®: {query}"
                    insights_response = agent.run(insights_query)
                    st.markdown(insights_response.content)
                
                except Exception as e:
                    st.error(f"æ¨ç†åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
    
    with col1:
        # Reasoning parameters
        st.info(f"ğŸ”§ æ¨ç†é…ç½®: çŸ¥è¯†æœç´¢ + åˆ†æå·¥å…· | åµŒå…¥æ¨¡å‹: text-embedding-3-large")
        
        # Show reasoning tools info
        with st.expander("â„¹ï¸ æ¨ç†å·¥å…·è¯´æ˜"):
            st.markdown("""
            **æ¨ç†å·¥å…·åŠŸèƒ½:**
            - ğŸ¤” **Think**: ç»“æ„åŒ–æ€è€ƒç©ºé—´
            - ğŸ” **Search**: çŸ¥è¯†åº“æœç´¢
            - ğŸ“Š **Analyze**: ç»“æœåˆ†æå·¥å…·
            
            **é€‚ç”¨åœºæ™¯:**
            - å¤æ‚çš„å¤šæ­¥éª¤åˆ†æ
            - éœ€è¦å¯¹æ¯”å¤šä¸ªåŒ–åˆç‰©
            - å®éªŒè®¾è®¡æ–¹æ¡ˆåˆ¶å®š
            - å®‰å…¨æ€§è¯„ä¼°
            """)

def main():
    """Main application entry point."""
    setup_page_config()
    
    # Header
    st.title("ğŸ§¬ ToxiRAG - è‚ç™Œæ¯’æ€§é¢„æµ‹ç³»ç»Ÿ")
    st.markdown("*åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„AIè¾…åŠ©åŠ¨ç‰©å®éªŒé¢„æµ‹å¹³å°*")
    
    # Sidebar configuration
    config = setup_sidebar()
    
    # Check API keys
    if not config["openai_api_key"] and not config["google_api_key"]:
        st.warning("âš ï¸ è¯·åœ¨ä¾§è¾¹æ é…ç½®è‡³å°‘ä¸€ä¸ªAPIå¯†é’¥ä»¥ç»§ç»­ä½¿ç”¨")
        return
    
    # File ingestion section
    with st.expander("ğŸ“ çŸ¥è¯†åº“ç®¡ç†", expanded=False):
        ingest_section(config)
    
    # Main tabs
    tab1, tab2 = st.tabs(["ğŸ¤– æ£€ç´¢ä¸å›ç­”ï¼ˆGPT-5ï¼‰", "ğŸ§  æ¨ç†å¯è§†åŒ–ï¼ˆOpenAI embedï¼‰"])
    
    with tab1:
        gpt5_reasoning_tab(config)
    
    with tab2:
        reasoning_visualization_tab(config)
    
    # Footer
    st.markdown("---")
    st.markdown(
        "ğŸ’¡ **æç¤º**: ä½¿ç”¨æ¨ç†å¯è§†åŒ–åŠŸèƒ½å¯ä»¥çœ‹åˆ°AIçš„æ€è€ƒè¿‡ç¨‹ï¼Œ"
        "é€‚åˆå¤æ‚çš„æ¯’ç†å­¦åˆ†æå’Œå®éªŒè®¾è®¡ã€‚"
    )

if __name__ == "__main__":
    main() 