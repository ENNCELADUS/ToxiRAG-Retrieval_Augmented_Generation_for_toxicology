"""
ToxiRAG Streamlit Application
Main interface for liver cancer toxicity prediction and experiment planning.
"""

import os
import streamlit as st
import tempfile
import asyncio
from pathlib import Path
from typing import Optional, List, Dict, Any

# Local imports (will be created)
try:
    from ingest.ingest_local import ingest_markdown_file
    from retriever.retriever import retrieve_relevant_docs
    from llm.agentic_pipeline import create_agentic_response
except ImportError:
    st.error("Missing local modules. Please ensure ingest/, retriever/, and llm/ modules are implemented.")

def setup_page_config():
    """Configure Streamlit page settings."""
    st.set_page_config(
        page_title="ToxiRAG - è‚ç™Œæ¯’æ€§é¢„æµ‹ç³»ç»Ÿ",
        page_icon="ğŸ§¬",
        layout="wide",
        initial_sidebar_state="expanded"
    )

def setup_sidebar() -> Dict[str, Any]:
    """Setup sidebar with API keys and model selection."""
    st.sidebar.title("ğŸ”§ é…ç½®è®¾ç½®")
    
    # API Keys section
    st.sidebar.subheader("API å¯†é’¥")
    openai_api_key = st.sidebar.text_input(
        "OpenAI API Key",
        type="password",
        value=os.getenv("OPENAI_API_KEY", ""),
        help="ç”¨äº GPT-5 Nano å’Œ OpenAI åµŒå…¥"
    )
    
    google_api_key = st.sidebar.text_input(
        "Google API Key", 
        type="password",
        value=os.getenv("GOOGLE_API_KEY", ""),
        help="ç”¨äº Gemini 2.5 Flash"
    )
    
    # Model selection
    st.sidebar.subheader("æ¨¡å‹é€‰æ‹©")
    llm_provider = st.sidebar.selectbox(
        "LLM æä¾›å•†",
        ["openai", "gemini"],
        help="é€‰æ‹©ä¸»è¦çš„è¯­è¨€æ¨¡å‹æä¾›å•†"
    )
    
    # Embedding provider
    embedding_provider = st.sidebar.selectbox(
        "åµŒå…¥æ¨¡å‹æä¾›å•†",
        ["openai"],  # Only OpenAI embeddings for now
        help="é€‰æ‹©æ–‡æœ¬åµŒå…¥æ¨¡å‹"
    )
    
    # Advanced settings
    st.sidebar.subheader("é«˜çº§è®¾ç½®")
    max_tokens = st.sidebar.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 100, 4000, 2000)
    temperature = st.sidebar.slider("æ¸©åº¦", 0.0, 1.0, 0.1)
    top_k_docs = st.sidebar.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡", 1, 20, 5)
    
    return {
        "openai_api_key": openai_api_key,
        "google_api_key": google_api_key,
        "llm_provider": llm_provider,
        "embedding_provider": embedding_provider,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_k_docs": top_k_docs
    }

async def run_async_agentic_response(query: str, config: Dict[str, Any], collection_name: str = "tcm_tox", use_reasoning_tools: bool = True):
    """Wrapper to run async agentic response in Streamlit."""
    try:
        response = await create_agentic_response(
            query=query,
            config=config,
            collection_name=collection_name,
            use_reasoning_tools=use_reasoning_tools
        )
        return response
    except Exception as e:
        st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
        return None

def ingest_section(config: Dict[str, Any]):
    """File upload and ingestion section."""
    st.subheader("ğŸ“ æ•°æ®æ‘„å–")
    
    # File upload
    uploaded_file = st.file_uploader(
        "ä¸Šä¼  Markdown çŸ¥è¯†æ–‡ä»¶",
        type=['md', 'txt'],
        help="ä¸Šä¼ åŒ…å«æ¯’ç†å­¦ç ”ç©¶æ•°æ®çš„ Markdown æ–‡ä»¶"
    )
    
    if uploaded_file is not None:
        # Save uploaded file temporarily
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as tmp_file:
            content = uploaded_file.read().decode('utf-8')
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸš€ å¼€å§‹æ‘„å–", key="ingest_button"):
                try:
                    with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶..."):
                        # Call ingestion function (async)
                        result = asyncio.run(ingest_markdown_file(
                            file_path=tmp_file_path,
                            collection_name="tcm_tox"
                        ))
                        st.success(f"âœ… æ–‡ä»¶æ‘„å–æˆåŠŸï¼å¤„ç†äº† {result.get('chunks', 0)} ä¸ªæ–‡æ¡£å—")
                except Exception as e:
                    st.error(f"âŒ æ‘„å–å¤±è´¥: {str(e)}")
                finally:
                    # Clean up temp file
                    if os.path.exists(tmp_file_path):
                        os.unlink(tmp_file_path)
        
        with col2:
            # Preview content
            st.text_area("æ–‡ä»¶é¢„è§ˆ", content[:500] + "..." if len(content) > 500 else content, height=100)

def gpt5_reasoning_tab(config: Dict[str, Any]):
    """GPT-5 retrieval and reasoning tab using new agentic pipeline."""
    st.header("ğŸ¤– æ£€ç´¢ä¸å›ç­”ï¼ˆGPT-5ï¼‰")
    
    # Check API keys
    if not config["openai_api_key"] and not config["google_api_key"]:
        st.error("æ— æ³•è®¾ç½®æ¨ç†ä»£ç†ã€‚è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®ã€‚")
        return
    
    # Query input
    query = st.text_area(
        "è¯·è¾“å…¥æ‚¨çš„æ¯’ç†å­¦é—®é¢˜:",
        placeholder="ä¾‹å¦‚: è¯·åˆ†ææŸä¸ªTCMåŒ–åˆç‰©å¯¹è‚ç™Œç»†èƒçš„æ¯’æ€§ä½œç”¨æœºåˆ¶...",
        height=100
    )
    
    col1, col2 = st.columns([3, 1])
    
    with col2:
        if st.button("ğŸ” åˆ†æ", key="gpt5_analyze"):
            if not query.strip():
                st.warning("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹")
                return
            
            with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶åˆ†æ..."):
                try:
                    # Use new agentic pipeline
                    response = asyncio.run(run_async_agentic_response(
                        query=query,
                        config=config,
                        collection_name="tcm_tox",
                        use_reasoning_tools=False  # Basic reasoning for this tab
                    ))
                    
                    if response and response.refusal_reason is None:
                        # Display results
                        st.subheader("ğŸ“Š åˆ†æç»“æœ")
                        st.markdown(response.response_text)
                        
                        # Display evidence pack info
                        if response.evidence_pack.citation_ids:
                            st.subheader("ğŸ“‹ è¯æ®å¼•ç”¨")
                            for citation in response.citations:
                                st.markdown(f"- {citation}")
                        
                        # Display retrieved sources
                        with st.expander("ğŸ“š å‚è€ƒæ–‡çŒ®æ¥æº"):
                            for i, doc in enumerate(response.evidence_pack.retrieved_docs, 1):
                                st.write(f"**æ¥æº {i}:** {doc.get('document_title', 'æœªçŸ¥æ ‡é¢˜')}")
                                st.write(f"**èŠ‚æ®µ:** {doc.get('section_type', 'æœªçŸ¥èŠ‚æ®µ')}")
                                st.write(f"**æ‘˜è¦:** {doc.get('content', '')[:200]}...")
                                if doc.get('source_page'):
                                    st.write(f"**é¡µé¢:** {doc.get('source_page')}")
                                st.write("---")
                        
                        # Display confidence and reasoning info
                        st.subheader("ğŸ” åˆ†æä¿¡æ¯")
                        st.write(f"**ç½®ä¿¡åº¦:** {response.confidence_score:.2f}")
                        st.write(f"**æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°:** {len(response.evidence_pack.retrieved_docs)}")
                        st.write(f"**å¼•ç”¨æ•°:** {len(response.citations)}")
                    
                    elif response and response.refusal_reason:
                        st.warning("âš ï¸ åˆ†æå—é™")
                        st.markdown(response.response_text)
                        st.write(f"**åŸå› :** {response.refusal_reason}")
                    
                    else:
                        st.error("åˆ†æå¤±è´¥ï¼Œè¯·é‡è¯•")
                
                except Exception as e:
                    st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
    
    with col1:
        # Show query parameters
        st.info(f"ğŸ”§ å½“å‰é…ç½®: {config['llm_provider'].upper()} | æ£€ç´¢æ–‡æ¡£æ•°: {config['top_k_docs']} | æ¸©åº¦: {config['temperature']}")

def reasoning_visualization_tab(config: Dict[str, Any]):
    """Reasoning visualization tab with advanced agentic pipeline."""
    st.header("ğŸ§  æ¨ç†å¯è§†åŒ–ï¼ˆé«˜çº§åˆ†æï¼‰")
    
    # Check API keys
    if not config["openai_api_key"] and not config["google_api_key"]:
        st.error("æ— æ³•è®¾ç½®çŸ¥è¯†ä»£ç†ã€‚è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®ã€‚")
        return
    
    # Query input
    query = st.text_area(
        "è¯·è¾“å…¥å¤æ‚çš„æ¯’ç†å­¦ç ”ç©¶é—®é¢˜:",
        placeholder="ä¾‹å¦‚: æ¯”è¾ƒä¸åŒTCMåŒ–åˆç‰©åœ¨è‚ç™Œæ²»ç–—ä¸­çš„æ¯’æ€§å·®å¼‚ï¼Œå¹¶æä¾›å®éªŒè®¾è®¡æ–¹æ¡ˆ...",
        height=100
    )
    
    col1, col2 = st.columns([3, 1])
    
    with col2:
        show_reasoning = st.checkbox("æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹", value=True)
        if st.button("ğŸ§© æ·±åº¦åˆ†æ", key="reasoning_analyze"):
            if not query.strip():
                st.warning("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹")
                return
            
            with st.spinner("æ­£åœ¨è¿›è¡Œæ·±åº¦æ¨ç†åˆ†æ..."):
                try:
                    # Use advanced agentic pipeline with reasoning tools
                    response = asyncio.run(run_async_agentic_response(
                        query=query,
                        config=config,
                        collection_name="tcm_tox",
                        use_reasoning_tools=True  # Advanced reasoning for this tab
                    ))
                    
                    if response and response.refusal_reason is None:
                        # Show reasoning steps if requested
                        if show_reasoning and response.reasoning_steps:
                            st.subheader("ğŸ” æ¨ç†è¿‡ç¨‹")
                            reasoning_container = st.container()
                            
                            with reasoning_container:
                                st.markdown("**æ¨ç†æ­¥éª¤:**")
                                for i, step in enumerate(response.reasoning_steps, 1):
                                    st.markdown(f"{i}. {step}")
                        
                        # Display main results
                        st.subheader("ğŸ“‹ åˆ†ææŠ¥å‘Š")
                        st.markdown(response.response_text)
                        
                        # Display evidence and citations
                        if response.evidence_pack.citation_ids:
                            st.subheader("ğŸ“‹ è¯æ®å¼•ç”¨")
                            for citation in response.citations:
                                st.markdown(f"- {citation}")
                        
                        # Advanced analysis metrics
                        st.subheader("ğŸ”¬ åˆ†æè¯¦æƒ…")
                        col_a, col_b, col_c = st.columns(3)
                        
                        with col_a:
                            st.metric("ç½®ä¿¡åº¦", f"{response.confidence_score:.2f}")
                        
                        with col_b:
                            st.metric("è¯æ®æ–‡æ¡£", len(response.evidence_pack.retrieved_docs))
                        
                        with col_c:
                            st.metric("å¼•ç”¨æ•°é‡", len(response.citations))
                        
                        # Query decomposition info
                        st.subheader("ğŸ§© æŸ¥è¯¢åˆ†è§£")
                        st.write("**æŸ¥è¯¢ç±»å‹åˆ†ç±»**: è‡ªåŠ¨è¯†åˆ«å¹¶åˆ†è§£ä¸ºå¤šä¸ªå­æŸ¥è¯¢")
                        st.write("**è¯æ®æ£€ç´¢**: å»é‡åçš„ç›¸å…³æ–‡æ¡£")
                        st.write("**æ¨ç†åˆæˆ**: åŸºäºè¯æ®çš„ç»“æ„åŒ–åˆ†æ")
                        
                        # Display retrieved sources with advanced info
                        with st.expander("ğŸ“š è¯¦ç»†æ–‡çŒ®æ¥æº"):
                            for i, doc in enumerate(response.evidence_pack.retrieved_docs, 1):
                                st.write(f"**æ–‡æ¡£ {i}:** {doc.get('document_title', 'æœªçŸ¥æ ‡é¢˜')}")
                                st.write(f"**èŠ‚æ®µç±»å‹:** {doc.get('section_type', 'æœªçŸ¥èŠ‚æ®µ')}")
                                if doc.get('vector_score'):
                                    st.write(f"**ç›¸ä¼¼åº¦å¾—åˆ†:** {doc.get('vector_score', 0):.3f}")
                                if doc.get('bm25_score'):
                                    st.write(f"**å…³é”®è¯å¾—åˆ†:** {doc.get('bm25_score', 0):.3f}")
                                if doc.get('combined_score'):
                                    st.write(f"**ç»¼åˆå¾—åˆ†:** {doc.get('combined_score', 0):.3f}")
                                st.write(f"**å†…å®¹æ‘˜è¦:** {doc.get('content', '')[:200]}...")
                                if doc.get('source_page'):
                                    st.write(f"**æºé¡µé¢:** {doc.get('source_page')}")
                                st.write("---")
                    
                    elif response and response.refusal_reason:
                        st.warning("âš ï¸ æ·±åº¦åˆ†æå—é™")
                        st.markdown(response.response_text)
                        st.write(f"**é™åˆ¶åŸå› :** {response.refusal_reason}")
                    
                    else:
                        st.error("æ·±åº¦åˆ†æå¤±è´¥ï¼Œè¯·é‡è¯•")
                
                except Exception as e:
                    st.error(f"æ¨ç†åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
    
    with col1:
        # Advanced reasoning parameters
        st.info(f"ğŸ”§ é«˜çº§é…ç½®: {config['llm_provider'].upper()} + çŸ¥è¯†æ¨ç†å·¥å…· | åµŒå…¥: text-embedding-3-large")
        
        # Show reasoning tools info
        with st.expander("â„¹ï¸ é«˜çº§æ¨ç†åŠŸèƒ½"):
            st.markdown("""
            **æ™ºèƒ½æŸ¥è¯¢åˆ†è§£:**
            - ğŸ§  **è‡ªåŠ¨åˆ†ç±»**: æœºåˆ¶/æ¯’æ€§/è®¾è®¡/å¯¹æ¯”/ä¸€èˆ¬
            - ğŸ” **å¤šè§’åº¦æ£€ç´¢**: å­æŸ¥è¯¢å¹¶è¡Œæœç´¢
            - ğŸ“Š **è¯æ®èšåˆ**: å»é‡å’Œç›¸å…³æ€§æ’åº
            
            **æ¨ç†å¢å¼ºåŠŸèƒ½:**
            - ğŸ¤” **ç»“æ„åŒ–æ€è€ƒ**: åˆ†æ­¥åˆ†æé—®é¢˜
            - ğŸ” **çŸ¥è¯†åº“æœç´¢**: æ··åˆæ£€ç´¢ç­–ç•¥
            - ğŸ“Š **è¯æ®è¯„ä¼°**: ç½®ä¿¡åº¦è®¡ç®—
            - ğŸ›¡ï¸ **å®‰å…¨é˜²æŠ¤**: æ‹’ç»å›ç­”ä¸å……åˆ†é—®é¢˜
            
            **é€‚ç”¨å¤æ‚åœºæ™¯:**
            - å¤šåŒ–åˆç‰©å¯¹æ¯”åˆ†æ
            - å®éªŒè®¾è®¡æ–¹æ¡ˆåˆ¶å®š
            - æœºåˆ¶ç ”ç©¶ç»¼åˆè¯„ä¼°
            - å®‰å…¨æ€§é£é™©åˆ†æ
            """)

def main():
    """Main application entry point."""
    setup_page_config()
    
    # Header
    st.title("ğŸ§¬ ToxiRAG - è‚ç™Œæ¯’æ€§é¢„æµ‹ç³»ç»Ÿ")
    st.markdown("*åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„AIè¾…åŠ©åŠ¨ç‰©å®éªŒé¢„æµ‹å¹³å°*")
    
    # Sidebar configuration
    config = setup_sidebar()
    
    # Check API keys
    if not config["openai_api_key"] and not config["google_api_key"]:
        st.warning("âš ï¸ è¯·åœ¨ä¾§è¾¹æ é…ç½®è‡³å°‘ä¸€ä¸ªAPIå¯†é’¥ä»¥ç»§ç»­ä½¿ç”¨")
        return
    
    # File ingestion section
    with st.expander("ğŸ“ çŸ¥è¯†åº“ç®¡ç†", expanded=False):
        ingest_section(config)
    
    # Main tabs
    tab1, tab2 = st.tabs(["ğŸ¤– æ£€ç´¢ä¸å›ç­”ï¼ˆGPT-5ï¼‰", "ğŸ§  æ¨ç†å¯è§†åŒ–ï¼ˆOpenAI embedï¼‰"])
    
    with tab1:
        gpt5_reasoning_tab(config)
    
    with tab2:
        reasoning_visualization_tab(config)
    
    # Footer
    st.markdown("---")
    st.markdown(
        "ğŸ’¡ **æç¤º**: ä½¿ç”¨æ¨ç†å¯è§†åŒ–åŠŸèƒ½å¯ä»¥çœ‹åˆ°AIçš„æ€è€ƒè¿‡ç¨‹ï¼Œ"
        "é€‚åˆå¤æ‚çš„æ¯’ç†å­¦åˆ†æå’Œå®éªŒè®¾è®¡ã€‚"
    )

if __name__ == "__main__":
    main() 