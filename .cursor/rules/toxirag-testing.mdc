---
description: Testing patterns and standards for ToxiRAG
alwaysApply: true
---

# ToxiRAG Testing Guidelines

## Test Data Standards

### Use Real Toxicology Data
- Always use actual examples from `data/summaries/肝癌.md` for test fixtures
- Include data from at least 3 different toxicology papers per test suite
- Use realistic measurements, units, and Chinese/English mixed content
- Include both successful cases and edge cases (missing data, unusual formats)

### Test Structure Organization
- Mirror main package structure in `tests/` directory
- One test file per main module (e.g., `test_retriever.py` for `retriever.py`)
- Group related tests in classes (e.g., `TestBM25Scorer`, `TestEvidencePack`)
- Use descriptive test method names that explain the scenario being tested

## Mocking Patterns

### External Dependencies
- Mock OpenAI embedding calls with `AsyncMock` but use real content
- Mock LanceDB connections but use realistic pandas DataFrames
- Mock HTTP clients but preserve actual response structures
- Never mock core business logic or domain-specific functions

### Async Testing
- Use `@pytest.mark.asyncio` for all async test methods
- Mock async functions with `AsyncMock`, not regular `Mock`
- Test error handling in async contexts (timeouts, connection failures)
- Verify async resource cleanup (connections, file handles)

## Test Coverage Requirements

### Core Functionality
- Test all normalization functions with realistic toxicology data
- Cover all retrieval scenarios (vector search, BM25, hybrid, filtering)
- Test evidence pack generation with proper citation formatting
- Validate all data parsing with actual markdown content

### Edge Cases
- Missing or malformed data fields
- Unicode and mixed-language content
- Very large or very small numerical values
- Empty search results and fallback behaviors
- Duplicate content detection and handling

### Integration Scenarios
- End-to-end flows from raw markdown to final citations
- Cross-module interactions (parser → normalizer → chunker → retriever)
- Configuration validation and environment variable handling
- Error propagation across module boundaries

## Test Data Management

### Fixtures and Samples
- Create reusable fixtures with common toxicology data patterns
- Use parametrized tests for testing multiple similar scenarios
- Include both positive and negative test cases
- Maintain test data versions alongside schema versions

### Performance Testing
- Include basic performance tests for retrieval operations
- Test with realistic dataset sizes (100s of documents)
- Verify memory usage doesn't grow unbounded
- Test concurrent access patterns for shared resources

## API Key Security for Testing

### Real API Key Usage
- **CRITICAL**: Only use real API keys for integration testing when necessary
- Always use conditional skipping with `@pytest.mark.skipif(not API_KEY)` for real API tests
- Prefer mocked tests for unit testing; use real APIs only for integration validation

### Security Protocol
- **FORGET API KEYS**: After using real API keys for testing, immediately clear them from memory/variables
- Never commit actual API keys to version control
- Use environment variables and .env files (with .env in .gitignore)
- Rotate API keys regularly if used in testing
- In production testing, use dedicated test API keys with limited quotas

### Example Safe Usage
```python
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
# Use for testing...
# After testing:
GOOGLE_API_KEY = None  # Clear from memory
del GOOGLE_API_KEY     # Remove variable
```