## M5 - Evaluation and E2E Test: COMPLETED ✅

### 🎯 **Core Deliverables**

1. **✅ Golden Questions Dataset** (`eval/golden_questions.yaml`)
   - 11 high-quality questions derived from actual sections in `data/summaries/肝癌.md`
   - Covers mechanism, design, data_table, and tumor_model query types
   - Real citations with document titles, section tags, source pages, and required phrases
   - Questions span 3 different papers: 玉米花粉多糖, 玛咖生物碱, and 理冲汤+5-FU studies

2. **✅ Evaluation Framework** (`eval/evaluator.py`)
   - Custom grounding score calculation (overlap-based)
   - Citation coverage validation against expected citations
   - Phrase coverage for required toxicology terms
   - Configurable scoring weights and thresholds
   - Detailed result reporting with JSON output

3. **✅ End-to-End Tests** (`tests/e2e/test_rag_flow.py`)
   - Complete pipeline testing: ingest → retrieve → answer → citations
   - Mocked embeddings and LLM tests for fast CI
   - Real integration tests (with API key guards)
   - Citation format validation (`[E1 · 机制研究结果]`)
   - Proper database isolation for testing

4. **✅ Coverage & CI Setup**
   - `pytest.ini` with 70% coverage target
   - CI-ready test commands in `plan.md`
   - Fast test subset (excluding slow/integration tests)
   - HTML, XML, and terminal coverage reports

5. **✅ Integration** 
   - Updated `scripts/eval_run.py` to use the evaluator
   - Async evaluation pipeline with proper error handling
   - Pass/fail thresholds for CI environments
   - Comprehensive logging and result statistics

### 🔧 **Technical Implementation**

- **Real Data Foundation**: Questions use verified sections from `肝癌.md` with actual page numbers
- **Multi-Paper Coverage**: Spans immune enhancement, direct cytotoxicity, and EMT mechanisms  
- **Citation Traceability**: Expected citations include `must_include_phrases` for validation
- **Flexible Evaluation**: Supports both custom metrics and future RAGAS integration
- **Test Robustness**: 85+ tests passing with proper mocking and database isolation

### 📊 **Quality Metrics**

- **Coverage Target**: ≥70% code coverage enforced
- **Test Suite**: Complete E2E pipeline validation
- **Citation Accuracy**: Format and content validation against gold standard
- **Performance**: Fast CI tests + comprehensive integration tests
- **Maintainability**: Clear separation of concerns and comprehensive documentation

### 🚀 **Usage Examples**

```bash
# Run evaluation against golden questions
python scripts/eval_run.py --eval-config eval/config.yaml --verbose

# Run all tests with coverage
pytest --maxfail=1

# Run fast CI tests only
pytest --maxfail=1 -m "not slow and not integration"
```

The evaluation framework is now ready to validate the quality of ToxiRAG's evidence-based responses against real toxicology study data, ensuring proper citation formatting and grounding in scientific evidence.